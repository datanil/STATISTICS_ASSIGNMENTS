{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1zowE4TaV13M7pFk2F0hV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.**\n","\n","### ***ANSWER :***"],"metadata":{"id":"yiX3q8TBjPS1"}},{"cell_type":"markdown","source":["Analysis of Variance (ANOVA) is a statistical method used to compare the means of two or more groups and determine if there are any significant differences among them. However, ANOVA relies on certain assumptions for its validity. Violating these assumptions can lead to inaccurate or unreliable results. The key assumptions for using ANOVA are:\n","\n","1. **Independence of observations:** Observations within each group or category should be independent of each other. This means that the values in one group should not be influenced by or related to the values in another group.\n","\n","2. **Normality:** The dependent variable (the one being measured) should follow a normal distribution in each group. Normality assumption is especially important when the sample sizes are small (typically, a sample size of around 30 is considered large enough for ANOVA to be robust to violations of normality).\n","\n","3. **Homogeneity of variance (homoscedasticity):** The variances of the dependent variable should be approximately equal across all groups. In other words, the spread or dispersion of data points in each group should be similar.\n","\n","4. **Equality of group sizes (for one-way ANOVA):** For a one-way ANOVA (comparing means across multiple groups), the sample sizes should be equal or at least roughly balanced. In some cases, unequal group sizes can still be accommodated, but having very different group sizes may affect the validity of the results.\n","\n","***Examples of violations that could impact the validity of ANOVA results:***\n","\n","1. **Non-independence of observations:** If data points in one group are influenced by or dependent on data points in another group, the independence assumption is violated. For example, if multiple measurements are taken from the same individual over time, these measurements may not be independent and can bias the ANOVA results.\n","\n","2. **Non-normality:** If the data does not follow a normal distribution within each group, the results of ANOVA may not be accurate. This can happen when dealing with small sample sizes or when extreme outliers are present.\n","\n","3. **Heteroscedasticity:** If the variance of the dependent variable differs significantly across groups, the assumption of homogeneity of variance is violated. This can lead to unequal weighting of groups and affect the overall significance of the ANOVA results.\n","\n","4. **Unequal group sizes:** Although ANOVA can tolerate some imbalance in group sizes, having highly unequal group sizes can lead to biased results and reduced statistical power.\n","\n","When these assumptions are violated, alternative statistical tests or data transformations might be necessary to draw valid conclusions. For example, non-parametric tests like the Kruskal-Wallis test can be used when normality assumptions are not met, and transformations like the logarithmic or square root transformation may help address issues related to heteroscedasticity or non-normality. Always visually inspecting the data and using diagnostic tests can aid in identifying potential violations and selecting appropriate analyses."],"metadata":{"id":"GpxiOPQi1UMH"}},{"cell_type":"markdown","source":["### **Q2. What are the three types of ANOVA, and in what situations would each be used?**\n","\n","### ***ANSWER :***"],"metadata":{"id":"rSqEf963jRto"}},{"cell_type":"markdown","source":["The three main types of ANOVA are:\n","\n","1. **One-Way ANOVA (Analysis of Variance):**\n","   One-Way ANOVA is used when you have one categorical independent variable (also known as a factor) and one continuous dependent variable. The categorical variable divides the data into two or more groups, and the continuous variable is measured for each group. The purpose of One-Way ANOVA is to determine if there are any significant differences in the means of the dependent variable among the different groups.\n","\n","   Example situations for using One-Way ANOVA:\n","   - Comparing the average test scores of students from different schools (with schools as the groups).\n","   - Analyzing the effect of different treatments on a specific medical condition.\n","\n","2. **Two-Way ANOVA:**\n","   Two-Way ANOVA is used when you have two categorical independent variables and one continuous dependent variable. This type of ANOVA allows you to explore the interaction between the two independent variables and their combined effect on the dependent variable.\n","\n","   Example situations for using Two-Way ANOVA:\n","   - Studying the impact of two different factors (e.g., gender and age group) on salary levels.\n","   - Investigating the influence of both type of diet and exercise regimen on weight loss.\n","\n","3. **Repeated Measures ANOVA (or One-Way Within-Subjects ANOVA):**\n","   Repeated Measures ANOVA is used when you have a single group of participants who are measured on the same dependent variable under different conditions or at multiple time points. This type of ANOVA allows you to examine within-subject effects and assess how the different conditions or time points influence the dependent variable.\n","\n","   Example situations for using Repeated Measures ANOVA:\n","   - Evaluating the effectiveness of a memory training program by measuring participants' memory performance before training, immediately after, and one month after the training.\n","   - Analyzing the effects of different levels of workload on participants' stress levels by measuring their stress levels at different time points during a simulation.\n","\n"," ***The choice of ANOVA type depends on the number of independent variables and the design of the study. One-Way ANOVA is suitable for comparing means across multiple groups, Two-Way ANOVA is used when there are two categorical independent variables, and Repeated Measures ANOVA is employed when dealing with within-subject designs with multiple measurements over time or conditions.***"],"metadata":{"id":"Aa7x3NkB1iWv"}},{"cell_type":"markdown","source":["### **Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?**\n","\n","### ***ANSWER :***"],"metadata":{"id":"HZsdWxu8jTTy"}},{"cell_type":"markdown","source":["Partitioning of variance in ANOVA refers to the division of the total variance in the data into different components or sources of variation. These components represent the variability attributed to various factors or sources in the experimental design. Understanding this concept is crucial because it allows researchers to identify the contributions of different factors to the overall variability in the data and to assess the significance of these factors in explaining the observed differences among groups.\n","\n","In a One-Way ANOVA, the total variance in the data is partitioned into two main components:\n","\n","1. **Between-Groups Variance:** This component represents the variation between the group means. It measures how much the means of the different groups differ from each other. A larger between-groups variance indicates greater differences between the group means, which suggests that the factor being tested (e.g., different treatments, categories, or conditions) has a significant effect on the dependent variable.\n","\n","2. **Within-Groups Variance:** This component represents the variation within each group. It measures how much the individual data points within each group differ from their respective group mean. A larger within-groups variance indicates more variability within groups, which might be due to random fluctuations or measurement errors.\n","\n","The F-ratio (F-statistic) in ANOVA is calculated by dividing the between-groups variance by the within-groups variance. A high F-ratio suggests that the differences between group means are significant compared to the within-group variability, indicating that the factor being tested is likely influencing the dependent variable.\n","\n","The concept of partitioning of variance is important because it helps researchers to:\n","\n","1. **Assess the significance of the factor(s):** By comparing the between-groups and within-groups variance, researchers can determine if the observed differences between groups are statistically significant or simply due to random fluctuations.\n","\n","2. **Understand the effect size:** The proportion of between-groups variance to the total variance (total sum of squares) provides an effect size measure. A larger proportion suggests a stronger effect of the factor(s) on the dependent variable.\n","\n","3. **Identify potential sources of variation:** By partitioning the variance, researchers can gain insights into which factors or variables contribute more to the observed variability in the data.\n","\n","4. **Make informed decisions:** Understanding the partitioning of variance helps researchers interpret ANOVA results correctly and make informed decisions based on the significance and effect size of the factors being studied.\n","\n","***Partitioning of variance is a fundamental concept in ANOVA that aids in understanding the contributions of different factors to the variability in the data and determining the statistical significance of these factors. It plays a central role in the interpretation of ANOVA results and allows researchers to draw meaningful conclusions from their analyses.***"],"metadata":{"id":"XUnqdH0z1szp"}},{"cell_type":"markdown","source":["### **Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?**\n","\n","### ***ANSWER :***"],"metadata":{"id":"KEm74NrCjU9j"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import f_oneway\n","\n","# Assume you have data for three groups: group1, group2, and group3\n","# Replace these with your actual data arrays for each group\n","\n","group1 = [15, 18, 20, 22, 17]\n","group2 = [25, 28, 30, 35, 32]\n","group3 = [10, 14, 12, 9, 11]\n","\n","# Combine all the data into a single array for easier calculations\n","data = np.concatenate([group1, group2, group3])\n","\n","# Calculate the grand mean (overall mean)\n","grand_mean = np.mean(data)\n","\n","# Calculate the total sum of squares (SST)\n","SST = np.sum((data - grand_mean) ** 2)\n","\n","# Perform one-way ANOVA to get the explained sum of squares (SSE)\n","f_statistic, p_value = f_oneway(group1, group2, group3)\n","SSE = f_statistic * (len(group1) + len(group2) + len(group3) - 3)  # Degrees of freedom for groups = k-1\n","\n","# Calculate the residual sum of squares (SSR)\n","SSR = SST - SSE\n","\n","print(\"Total Sum of Squares (SST):\", SST)\n","print(\"Explained Sum of Squares (SSE):\", SSE)\n","print(\"Residual Sum of Squares (SSR):\", SSR)\n","print(\"F-statistic:\", f_statistic)\n","print(\"p-value:\", p_value)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTvX33_j2iHT","executionInfo":{"status":"ok","timestamp":1690285288567,"user_tz":-330,"elapsed":850,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"f4630e1c-5e1d-40ee-fd45-aec8e61e2d54"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Sum of Squares (SST): 1001.7333333333333\n","Explained Sum of Squares (SSE): 635.1058823529412\n","Residual Sum of Squares (SSR): 366.62745098039215\n","F-statistic: 52.925490196078435\n","p-value: 1.1145210562966572e-06\n"]}]},{"cell_type":"markdown","source":["### **Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?**\n","\n","### ***ANSWER :***"],"metadata":{"id":"Ew37scJIjXUw"}},{"cell_type":"markdown","source":["let's assume you have data for two independent variables, factor_A and factor_B, and a dependent variable y. Here's how you can calculate the main effects and interaction effects using statsmodels:"],"metadata":{"id":"CQanLvSv3LsX"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from statsmodels.formula.api import ols\n","\n","# Assume you have data for factor_A, factor_B, and the dependent variable y\n","# Replace these with your actual data arrays for each variable\n","\n","factor_A = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n","factor_B = [10, 10, 10, 10, 10, 20, 20, 20, 20, 20]\n","y = [15, 18, 22, 25, 28, 10, 14, 18, 22, 26]\n","\n","# Create a DataFrame to use with statsmodels\n","df = pd.DataFrame({'factor_A': factor_A, 'factor_B': factor_B, 'y': y})\n","\n","# Fit the two-way ANOVA model\n","model = ols('y ~ factor_A + factor_B + factor_A:factor_B', data=df).fit()\n","\n","# Print the ANOVA table and summary of the model\n","print(\"ANOVA Table:\")\n","print(sm.stats.anova_lm(model))\n","print(\"\\nModel Summary:\")\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hDqhvb6A3Pg9","executionInfo":{"status":"ok","timestamp":1690285456482,"user_tz":-330,"elapsed":10,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"c0dd91bd-6100-4a29-c1db-83fe7ad042d5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ANOVA Table:\n","                    df  sum_sq  mean_sq       F        PR(>F)\n","factor_A           1.0  266.45   266.45  5329.0  4.447170e-10\n","factor_B           1.0   32.40    32.40   648.0  2.421421e-07\n","factor_A:factor_B  1.0    2.45     2.45    49.0  4.234833e-04\n","Residual           6.0    0.30     0.05     NaN           NaN\n","\n","Model Summary:\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.999\n","Model:                            OLS   Adj. R-squared:                  0.999\n","Method:                 Least Squares   F-statistic:                     2009.\n","Date:                Tue, 25 Jul 2023   Prob (F-statistic):           2.15e-09\n","Time:                        11:44:15   Log-Likelihood:                 3.3434\n","No. Observations:                  10   AIC:                             1.313\n","Df Residuals:                       6   BIC:                             2.524\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","=====================================================================================\n","                        coef    std err          t      P>|t|      [0.025      0.975]\n","-------------------------------------------------------------------------------------\n","Intercept            17.4000      0.524     33.180      0.000      16.117      18.683\n","factor_A              2.6000      0.158     16.444      0.000       2.213       2.987\n","factor_B             -0.5700      0.033    -17.186      0.000      -0.651      -0.489\n","factor_A:factor_B     0.0700      0.010      7.000      0.000       0.046       0.094\n","==============================================================================\n","Omnibus:                        3.932   Durbin-Watson:                   2.667\n","Prob(Omnibus):                  0.140   Jarque-Bera (JB):                0.919\n","Skew:                           0.577   Prob(JB):                        0.632\n","Kurtosis:                       3.933   Cond. No.                         420.\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:1736: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n","  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"]}]},{"cell_type":"markdown","source":["# **OR**"],"metadata":{"id":"PF-0qX7H3-L9"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import f_oneway\n","\n","# Assume you have data for two independent variables, factor A and factor B,\n","# and a dependent variable y. Replace these with your actual data arrays.\n","\n","factor_A = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n","factor_B = [10, 10, 10, 10, 10, 20, 20, 20, 20, 20]\n","y = [15, 18, 22, 25, 28, 10, 14, 18, 22, 26]\n","\n","# Perform the two-way ANOVA\n","f_statistic_A, p_value_A = f_oneway(y[:5], y[5:], factor_A[:5], factor_A[5:])\n","f_statistic_B, p_value_B = f_oneway(y[:5], y[5:], factor_B[:5], factor_B[5:])\n","f_statistic_interaction, p_value_interaction = f_oneway(y[:5], y[5:], factor_A[:5], factor_A[5:], factor_B[:5], factor_B[5:])\n","\n","print(\"Main Effect of Factor A:\")\n","print(\"F-statistic:\", f_statistic_A)\n","print(\"p-value:\", p_value_A)\n","\n","print(\"\\nMain Effect of Factor B:\")\n","print(\"F-statistic:\", f_statistic_B)\n","print(\"p-value:\", p_value_B)\n","\n","print(\"\\nInteraction Effect between Factor A and Factor B:\")\n","print(\"F-statistic:\", f_statistic_interaction)\n","print(\"p-value:\", p_value_interaction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VWjfGA6V3mAA","executionInfo":{"status":"ok","timestamp":1690285536426,"user_tz":-330,"elapsed":600,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"467126df-6472-4ddd-aad4-0d35d5ae1b4f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Main Effect of Factor A:\n","F-statistic: 26.622406639004144\n","p-value: 1.8546234111986342e-06\n","\n","Main Effect of Factor B:\n","F-statistic: 7.877166914314019\n","p-value: 0.0018856225909745406\n","\n","Interaction Effect between Factor A and Factor B:\n","F-statistic: 29.543568464730278\n","p-value: 1.6378829151071226e-09\n"]}]},{"cell_type":"markdown","source":["### **Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?**\n","\n","### ***ANSWER :***"],"metadata":{"id":"dpduvfKyjY-Q"}},{"cell_type":"markdown","source":["In a one-way ANOVA, the F-statistic is used to test the null hypothesis that the means of all the groups are equal. If the obtained F-statistic is sufficiently large and the corresponding p-value is smaller than the chosen significance level (often 0.05), we can reject the null hypothesis. The rejection of the null hypothesis implies that there are significant differences between at least two of the groups' means.\n","\n","In your case, the F-statistic is 5.23, and the p-value is 0.02. Since the p-value (0.02) is less than the common significance level of 0.05, we can conclude that there are significant differences between the group means. The probability of obtaining such a large F-statistic (or even larger) under the assumption of equal group means is only 2%, which is smaller than the significance level of 5%. Therefore, we reject the null hypothesis and accept the alternative hypothesis that at least one group mean is different from the others.\n","\n","**Interpreting the results:**\n",">\n","The results of the one-way ANOVA indicate that the factor (or treatment) being studied has a statistically significant effect on the dependent variable. However, the ANOVA does not tell us which specific groups differ from each other; it only indicates that there is a difference somewhere among the groups.\n","\n","To identify which groups are different, you can perform post-hoc tests (e.g., Tukey's HSD test or Bonferroni correction) or pairwise comparisons. These tests will help you pinpoint which specific groups have significantly different means from one another.\n","\n","Keep in mind that the effect size is also important for interpretation. In addition to conducting post-hoc tests, you may want to calculate effect size measures (e.g., eta-squared, Cohen's d) to quantify the practical significance of the differences between the groups.\n","\n"],"metadata":{"id":"VucjyTbR4IYT"}},{"cell_type":"markdown","source":["### **Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?**\n","\n","### ***ANSWER :***"],"metadata":{"id":"QKog0jARjaUx"}},{"cell_type":"markdown","source":["Handling missing data in a repeated measures ANOVA is an important consideration to ensure valid and reliable results. There are several methods to handle missing data, each with its potential consequences. Here are some common approaches:\n","\n","1. **Complete Case Analysis (Listwise Deletion):**\n","   This method involves excluding any participant or case with missing data on any variable involved in the analysis. It is the simplest approach but can lead to reduced sample size and potential loss of statistical power. It assumes that the data is missing completely at random (MCAR), which may not always be a valid assumption.\n","\n","2. **Mean Imputation:**\n","   Missing values in each condition or time point are replaced with the mean of the available data for that condition or time point. This method may introduce bias and artificially reduce variability, leading to inaccurate estimates of the treatment effects.\n","\n","3. **Last Observation Carried Forward (LOCF):**\n","   Missing values are replaced with the last observed value for that participant. This approach assumes that data is missing due to random fluctuations, but it can distort the results and artificially inflate treatment effects.\n","\n","4. **Multiple Imputation:**\n","   Multiple imputation generates several plausible imputations for the missing data based on the observed data and incorporates the uncertainty of imputation into the analysis. This method can provide more robust results when data is missing at random (MAR), but it requires careful implementation and may be computationally intensive.\n","\n","5. **Model-Based Imputation:**\n","   This approach uses a statistical model to predict missing values based on other variables in the data. Imputed values are drawn from the model's predicted distribution. Model-based imputation may be effective when there are systematic patterns of missing data, but it relies on the validity of the underlying model.\n","\n","***Potential consequences of using different methods:***\n","\n","- Complete Case Analysis (Listwise Deletion): Reduced sample size, loss of statistical power, and potential bias if data is not missing completely at random.\n","\n","- Mean Imputation: Underestimation of standard errors, inflated significance levels, and biased parameter estimates.\n","\n","- LOCF: Potential distortion of treatment effects, especially if missing data is related to participants dropping out due to negative treatment effects.\n","\n","- Multiple Imputation: More accurate parameter estimates and standard errors when data is missing at random, but it may be computationally demanding.\n","\n","- Model-Based Imputation: Effectiveness depends on the validity of the model used for imputation. If the model is misspecified, it can lead to biased results.\n","\n","Selecting an appropriate method for handling missing data requires understanding the underlying mechanisms of missingness and consideration of the assumptions and potential biases introduced by each method. It is crucial to be cautious in interpreting the results when handling missing data, as the chosen approach can influence the validity and generalizability of the findings."],"metadata":{"id":"uqA5cKc14TK8"}},{"cell_type":"markdown","source":["### **Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.**\n","\n","### ***ANSWER :***"],"metadata":{"id":"Uk1vpGIGjb8p"}},{"cell_type":"markdown","source":["After conducting an analysis of variance (ANOVA) and finding a significant overall effect, post-hoc tests are used to make pairwise comparisons between groups to determine which specific group differences are statistically significant. Several common post-hoc tests exist, each with different strengths and assumptions. Some common post-hoc tests include:\n","\n","1. **Tukey's Honestly Significant Difference (HSD) test:**\n","   Tukey's HSD is widely used when comparing all possible pairs of groups. It controls the family-wise error rate, making it suitable for situations where multiple pairwise comparisons are made. Tukey's HSD is appropriate when the sample sizes are equal across groups and variances are approximately equal.\n","\n","2. **Bonferroni correction:**\n","   The Bonferroni correction adjusts the significance level for each pairwise comparison to control the family-wise error rate. It is straightforward and conservative but may become overly stringent when there are many comparisons, leading to reduced power.\n","\n","3. **Dunnett's test:**\n","   Dunnett's test is useful when comparing several treatment groups to a single control group. It protects against inflation of the Type I error rate, making it more powerful than the Bonferroni correction for this specific situation.\n","\n","4. **Scheffe's test:**\n","   Scheffe's test is a conservative post-hoc test that can handle unequal sample sizes and complex designs. It is appropriate when there are many comparisons, but it may lack power compared to other post-hoc tests.\n","\n","5. **Fisher's Least Significant Difference (LSD) test:**\n","   Fisher's LSD is the simplest post-hoc test, and it is used when sample sizes are equal and variances are approximately equal. However, it does not control the family-wise error rate, making it more likely to produce false positives when conducting multiple comparisons.\n","\n","***Example of a situation where a post-hoc test might be necessary:***\n","\n","Suppose a researcher conducts a study to compare the effectiveness of four different treatments (A, B, C, and D) for reducing anxiety levels in patients. After performing a one-way ANOVA on the data, the researcher finds a significant overall effect, indicating that the treatments have different effects on anxiety levels.\n","\n","To determine which specific treatments significantly differ from each other, the researcher would conduct post-hoc tests. Tukey's HSD or Scheffe's test could be appropriate choices to make multiple pairwise comparisons among the treatment groups. These post-hoc tests would help identify which treatments have significantly different effects on anxiety levels and provide more detailed insights into the treatment efficacy. The choice of the specific post-hoc test would depend on factors such as the sample sizes, variance assumptions, and the number of comparisons being made."],"metadata":{"id":"SFuib7TU4aPm"}},{"cell_type":"markdown","source":["### **Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results.**\n","\n","### ***ANSWER :***"],"metadata":{"id":"LSwR3mR9jeFa"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import f_oneway\n","\n","# Assuming you have weight loss data for each diet: A, B, and C\n","# Replace these with your actual data arrays for each diet\n","\n","diet_A = [4.5, 5.2, 6.1, 4.9, 3.8, 5.5, 5.7, 4.3, 4.8, 5.9,\n","          5.2, 5.4, 4.7, 5.1, 5.3, 5.6, 4.9, 4.6, 5.0, 6.0,\n","          4.7, 5.8, 6.3, 5.4, 5.0, 5.2, 4.9, 5.6, 5.3, 5.0,\n","          6.2, 5.9, 5.7, 6.1, 5.3, 4.5, 5.0, 5.8, 5.6, 4.8,\n","          5.5, 5.3, 5.1, 5.4, 4.7, 4.6, 5.7, 6.0, 5.9, 5.2]\n","\n","diet_B = [2.9, 3.5, 3.1, 2.8, 3.0, 3.2, 3.4, 3.6, 2.7, 2.5,\n","          3.1, 3.0, 2.8, 3.5, 3.2, 3.6, 3.4, 2.9, 3.3, 2.6,\n","          3.3, 2.7, 2.9, 3.1, 3.2, 3.4, 2.8, 3.0, 3.3, 3.5,\n","          2.6, 3.2, 3.4, 2.7, 2.9, 3.0, 3.1, 3.3, 3.6, 3.4,\n","          2.8, 3.0, 3.5, 3.2, 2.9, 2.7, 3.1, 2.6, 3.3, 2.8]\n","\n","diet_C = [1.8, 2.1, 2.4, 1.9, 1.7, 2.2, 1.6, 2.0, 1.9, 1.5,\n","          2.3, 1.8, 2.5, 2.0, 2.1, 2.2, 2.4, 1.7, 1.6, 2.0,\n","          2.3, 2.2, 2.1, 1.9, 2.5, 2.0, 1.8, 2.3, 1.6, 1.7,\n","          1.9, 1.8, 2.1, 2.2, 1.6, 2.3, 2.5, 1.7, 2.0, 2.1,\n","          1.8, 2.2, 1.9, 2.0, 1.7, 1.6, 2.3, 2.5, 1.9, 2.1]\n","\n","# Combine the data into a single array for the one-way ANOVA\n","data = np.concatenate([diet_A, diet_B, diet_C])\n","\n","# Create corresponding group labels (e.g., A: 0, B: 1, C: 2)\n","group_labels = ['A'] * len(diet_A) + ['B'] * len(diet_B) + ['C'] * len(diet_C)\n","\n","# Perform the one-way ANOVA\n","f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n","\n","print(\"F-statistic:\", f_statistic)\n","print(\"p-value:\", p_value)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmcvktzY4m9A","executionInfo":{"status":"ok","timestamp":1690285820194,"user_tz":-330,"elapsed":644,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"98efd4b2-0b32-457c-9746-0b185cd1735b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["F-statistic: 894.5786885963022\n","p-value: 5.101359628292405e-83\n"]}]},{"cell_type":"markdown","source":["**Interpretation of results:**\n","***If the p-value is less than the chosen significance level (e.g., 0.05), we can reject the null hypothesis and conclude that there are significant differences in the mean weight loss between at least two of the diets (A, B, or C). If the p-value is greater than 0.05, you fail to reject the null hypothesis, indicating that there is insufficient evidence to conclude that there are significant differences between the mean weight loss of the three diets.***"],"metadata":{"id":"uoI_gdUe41mo"}},{"cell_type":"markdown","source":["### **Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs.experienced). Report the F-statistics and p-values, and interpret the results.**\n","\n","### ***ANSWER :***"],"metadata":{"id":"ewqol5Rsjgn6"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import f_oneway\n","\n","# Assume you have data for software programs (A, B, C), employee experience (novice vs. experienced),\n","# and the time it takes each employee to complete the task\n","# Replace these with your actual data arrays for each variable\n","\n","software = ['A', 'B', 'C'] * 10\n","experience = ['Novice'] * 15 + ['Experienced'] * 15\n","time_taken = [12, 10, 14, 11, 9, 13, 15, 16, 11, 12,\n","              8, 10, 9, 11, 12, 14, 15, 16, 12, 13,\n","              14, 12, 15, 13, 11, 9, 10, 12, 13, 11]\n","\n","# Perform the two-way ANOVA\n","f_statistic_software, p_value_software = f_oneway(time_taken[0:10], time_taken[10:20], time_taken[20:30])\n","f_statistic_experience, p_value_experience = f_oneway(time_taken[0:15], time_taken[15:30])\n","f_statistic_interaction, p_value_interaction = f_oneway(time_taken[0:5], time_taken[5:10], time_taken[10:15],\n","                                                       time_taken[15:20], time_taken[20:25], time_taken[25:30])\n","\n","print(\"Main Effect of Software Programs:\")\n","print(\"F-statistic:\", f_statistic_software)\n","print(\"p-value:\", p_value_software)\n","\n","print(\"\\nMain Effect of Employee Experience:\")\n","print(\"F-statistic:\", f_statistic_experience)\n","print(\"p-value:\", p_value_experience)\n","\n","print(\"\\nInteraction Effect between Software Programs and Employee Experience:\")\n","print(\"F-statistic:\", f_statistic_interaction)\n","print(\"p-value:\", p_value_interaction)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f6b6k1wc5Ihe","executionInfo":{"status":"ok","timestamp":1690285949229,"user_tz":-330,"elapsed":930,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"cd8593ae-fe5b-400f-f011-0a8d0fcfadde"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Main Effect of Software Programs:\n","F-statistic: 0.060402684563758385\n","p-value: 0.9415122147685216\n","\n","Main Effect of Employee Experience:\n","F-statistic: 2.1567164179104474\n","p-value: 0.15309356214651068\n","\n","Interaction Effect between Software Programs and Employee Experience:\n","F-statistic: 4.18\n","p-value: 0.0071280014947662056\n"]}]},{"cell_type":"markdown","source":["time_taken represents the time it takes each employee to complete the task, software represents the software programs (A, B, C) to which employees were assigned, and experience represents the employee experience level (novice vs. experienced).\n","\n","The two-way ANOVA is conducted by dividing the data into groups based on the levels of each factor (software and experience) and then performing one-way ANOVAs on each group to calculate the main effects of software and experience. Lastly, you can perform another one-way ANOVA on the groups formed by the combination of software and experience to calculate the interaction effect between the two factors.\n","\n","**Interpretation of results:**\n","\n","If the p-value for the main effect of Software Programs is less than the chosen significance level (e.g., 0.05), we can reject the null hypothesis and conclude that there are significant differences in the average time taken to complete the task between at least two of the software programs.\n","If the p-value for the main effect of Employee Experience is less than the chosen significance level (e.g., 0.05), you can reject the null hypothesis and conclude that there are significant differences in the average time taken to complete the task between novice and experienced employees.\n","If the p-value for the interaction effect between Software Programs and Employee Experience is less than the chosen significance level (e.g., 0.05), you can reject the null hypothesis and conclude that there is a significant interaction between the software programs and employee experience, meaning their combined effect is different from the sum of their individual effects"],"metadata":{"id":"tG5z_Ci35YPs"}},{"cell_type":"markdown","source":["### **Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other.**\n","\n","### ***ANSWER :***"],"metadata":{"id":"o_OfTBR_jiE2"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import ttest_ind\n","from statsmodels.stats.multicomp import pairwise_tukeyhsd\n","\n","# Assume you have data for test scores in the control and experimental groups\n","# Replace these with your actual data arrays for each group\n","\n","control_group = [85, 78, 90, 72, 88, 80, 76, 82, 75, 81,\n","                 77, 79, 86, 83, 89, 87, 80, 84, 78, 81,\n","                 84, 79, 75, 82, 86, 80, 77, 78, 79, 83,\n","                 81, 85, 88, 80, 82, 75, 78, 76, 81, 77,\n","                 89, 83, 80, 86, 78, 84, 87, 85, 80, 83]\n","\n","experimental_group = [88, 95, 92, 98, 90, 93, 89, 94, 96, 91,\n","                      90, 89, 92, 95, 91, 93, 96, 89, 90, 92,\n","                      93, 97, 94, 88, 91, 89, 96, 92, 90, 93,\n","                      95, 90, 94, 93, 91, 97, 88, 92, 94, 89,\n","                      96, 89, 93, 90, 92, 95, 97, 90, 91, 92]\n","\n","# Perform the two-sample t-test\n","t_statistic, p_value = ttest_ind(control_group, experimental_group)\n","\n","print(\"Two-Sample t-test:\")\n","print(\"t-statistic:\", t_statistic)\n","print(\"p-value:\", p_value)\n","\n","# Perform Tukey's HSD test for post-hoc multiple comparisons\n","data = np.array(control_group + experimental_group)\n","groups = np.array(['Control'] * len(control_group) + ['Experimental'] * len(experimental_group))\n","tukey_results = pairwise_tukeyhsd(data, groups)\n","print(\"\\nTukey's HSD Test:\")\n","print(tukey_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wnzx_kX_5_mS","executionInfo":{"status":"ok","timestamp":1690286168857,"user_tz":-330,"elapsed":1640,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"e880b694-35b9-4505-cb8b-b06a34246b0c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Two-Sample t-test:\n","t-statistic: -15.161451985791816\n","p-value: 1.9108030540809588e-27\n","\n","Tukey's HSD Test:\n","   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n","=========================================================\n"," group1    group2    meandiff p-adj lower   upper  reject\n","---------------------------------------------------------\n","Control Experimental    10.84   0.0 9.4212 12.2588   True\n","---------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["***Interpretation of results:***\n","\n","- In the output of the two-sample t-test, if the p-value is less than the chosen significance level (e.g., 0.05), we can reject the null hypothesis and conclude that there is a significant difference in test scores between the control group (traditional teaching method) and the experimental group (new teaching method).\n","- The Tukey's HSD test results will show which specific group(s) differ significantly from each other. It will provide confidence intervals and p-values for all pairwise comparisons between the control and experimental groups.\n","- For example, if the two-sample t-test yields a significant p-value (e.g., p < 0.05), and the Tukey's HSD test indicates that the control group and experimental group differ significantly, you can interpret the results as follows:\n","\n","\"The two-sample t-test indicates a significant difference in test scores between the control group (traditional teaching method) and the experimental group (new teaching method). Further analysis using Tukey's HSD test shows that the experimental group has significantly higher test scores compared to the control group.\""],"metadata":{"id":"Fk_7GYRM6Y-C"}},{"cell_type":"markdown","source":["### **Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other.**\n","\n","### ***ANSWER :***"],"metadata":{"id":"3Bi79VDmjkJL"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.stats import f_oneway\n","import pandas as pd\n","\n","# Assume you have daily sales data for Store A, Store B, and Store C\n","# Replace these with your actual data arrays for each store\n","\n","store_A_sales = [100, 110, 95, 105, 120, 130, 115, 105, 125, 105,\n","                 100, 105, 115, 120, 130, 110, 105, 120, 125, 105,\n","                 100, 115, 100, 110, 95, 105, 120, 130, 115, 105]\n","\n","store_B_sales = [95, 100, 85, 90, 100, 110, 95, 105, 100, 110,\n","                 95, 100, 105, 110, 115, 90, 85, 100, 95, 100,\n","                 95, 100, 85, 90, 100, 110, 95, 105, 100, 110]\n","\n","store_C_sales = [80, 85, 70, 75, 85, 90, 80, 85, 75, 90,\n","                 80, 85, 80, 85, 90, 70, 75, 80, 85, 75,\n","                 80, 85, 70, 75, 85, 90, 80, 85, 75, 90]\n","\n","# Combine the data into a single array for the one-way ANOVA\n","data = np.concatenate([store_A_sales, store_B_sales, store_C_sales])\n","\n","# Create corresponding group labels (e.g., A: 0, B: 1, C: 2)\n","group_labels = ['Store A'] * len(store_A_sales) + ['Store B'] * len(store_B_sales) + ['Store C'] * len(store_C_sales)\n","\n","# Create a DataFrame for easier analysis\n","df = pd.DataFrame({'Store': group_labels, 'Sales': data})\n","\n","# Perform the one-way ANOVA\n","f_statistic, p_value = f_oneway(df[df['Store'] == 'Store A']['Sales'],\n","                                df[df['Store'] == 'Store B']['Sales'],\n","                                df[df['Store'] == 'Store C']['Sales'])\n","\n","print(\"One-Way ANOVA:\")\n","print(\"F-statistic:\", f_statistic)\n","print(\"p-value:\", p_value)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7Pdn5hY6q6G","executionInfo":{"status":"ok","timestamp":1690286343244,"user_tz":-330,"elapsed":618,"user":{"displayName":"Niladri Ghosh","userId":"06052891715292441946"}},"outputId":"9c9873bc-92eb-40a8-eb2a-9349690914f2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["One-Way ANOVA:\n","F-statistic: 97.20196712476427\n","p-value: 6.65618393827822e-23\n"]}]},{"cell_type":"markdown","source":["***If the obtained p-value from the one-way ANOVA is less than the chosen significance level (e.g., 0.05), we can reject the null hypothesis and conclude that there are significant differences in the average daily sales between at least two of the retail stores.***"],"metadata":{"id":"wwkWzqcy61Vs"}}]}